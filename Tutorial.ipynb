{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PDFbones import Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffec267",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This module is designed to quickly set up and optimize a supervised neural network using 1d or 2d data with 1 truth value for each data entry. Here I will demonstrate how to use the network and show that it works on 1d and 2d data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697d67e",
   "metadata": {},
   "source": [
    "## Intializing the model\n",
    "### import as Model and init with Model(*args)\n",
    "#### Args:\n",
    "      \n",
    "      - lins: (list of integers) the widths of the linear layers\n",
    "      - activation: the activation function to be applied after each linear layer. You can create your own (using pytorch rules) or use a pytorch one\n",
    "      - optimizer: a pytorch optimizer\n",
    "      - batch_size: (int) number of samples to be trained on at a time\n",
    "      - init_lr: (float) initial learning rate (can be decayed with lr_decay kwarg)\n",
    "      - data: (array) can be 1d or 2d data. Data should be stacked along dim = 0. i.e. if you have 20 images size 3x3, your array should have shape (20, 3, 3). It will be split into train/test automatically.\n",
    "      -truth: (array) 1d array containing truth values\n",
    "      - cost: cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92670049",
   "metadata": {},
   "source": [
    "## Import some sample data, in this case various qualities of wine as data and their ratings as truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wines = pd.read_csv('winequality-red.csv').to_numpy()\n",
    "#truth = wines[:, -1]\n",
    "#data = wines[:, :-1]\n",
    "truth = np.load('../HMF/Notebooks/nonzero/nonzerotruth.npy')\n",
    "data = np.load('../HMF/Notebooks/nonzero/nonzerodata.npy')\n",
    "Cinv = np.load('../HMF/Notebooks/nonzero/nonzeroCinv.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fefa2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2(predicted, true, Cinverse):\n",
    "    D = predicted - true\n",
    "    loss = torch.matmul(torch.matmul(D.unsqueeze(1), Cinverse), D.unsqueeze(2)).squeeze().squeeze()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d243e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example using only necessary arguments\n",
    "lins = [1000, 100, 20, ]\n",
    "activation = nn.PReLU\n",
    "optimizer = optim.Adam\n",
    "batch_size = 128\n",
    "init_lr = 5e-3\n",
    "#data and truth loaded in the previous cell\n",
    "cost = chi2\n",
    "\n",
    "\n",
    "model = Model(lins, activation, optimizer, batch_size, init_lr, data, truth, Cinv, cost )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb131d",
   "metadata": {},
   "source": [
    "## Once the model is loaded, we can print it's parameters using model.params()\n",
    "I will explain what all the parameters mean in due time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c270eec",
   "metadata": {},
   "source": [
    "## Run it using model.run()\n",
    "It is automatically set to print the loss every epoch and plot train/test vs truth every 3 epochs, but we can adjust that.\n",
    "The default stopping point is 100 epochs, but we can change that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca53cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.run(\n",
    "    plot_ev = 10, max_epochs = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.testout.shape, model.data[0].shape, model.data[3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8eb0d",
   "metadata": {},
   "source": [
    "## if you run this again without re-initializing the model, it will pick up where you left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb2428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c7a82",
   "metadata": {},
   "source": [
    "## params will update with each epoch. You can see that err (average value of the cost function) and derr (the difference between the last and second to last loss value) now have values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09180e4d",
   "metadata": {},
   "source": [
    "## Now let's talk kwargs (optional arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f25f1d",
   "metadata": {},
   "source": [
    "### You can make the learning rate dynamic\n",
    "#### And adjust it as a kwarg in model.run()\n",
    "    - lr_decay: (float, default 1) learning rate decay factor. ex. if init_lr = .7 and lr_decay = .5, after 1 update the  learning rate will be lr = .7*.5. \n",
    "    -lr_min: (float, default 1e-8) minimum learning rate applied when decaying (generally a good idea to put in ~.95)\n",
    "    -max_epochs: max number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cc6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.run( lr = 1e-3, lr_decay = .95, lr_min = 1e-8, max_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0bbd12",
   "metadata": {},
   "source": [
    "### Saving\n",
    "    One of the key elements of this system is that you can save past runs and compare their outcomes. When you start a new run, if you use the same convolution and linear layers and activation function from a previous run you will get a warning telling you which run number it was and the parameters of that run. You can also load past runs a continue running them with the option to not save, save and update the loaded file, or save in a new location.\n",
    "    -saving: (bool, default False) If you want to save, set to true\n",
    "    - run_num: (int, default None) set to the number of a past run to load it and pick up where it left off\n",
    "    -new_tar: (bool, default False) set to True if you would like to load an old run (using run_num) but save the new results in a new file\n",
    "    \n",
    "        Note: weight saving is currently commented out\n",
    "        \n",
    "    Let's try turning on saving. I will get a warning alerting me that another run used the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013da6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-4\n",
    "model = Model(lins, activation, optimizer, batch_size, init_lr, data, truth, Cinv, cost, \n",
    "             max_epochs = 20, lr_decay = .9, saving = True)\n",
    "model.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c3dae",
   "metadata": {},
   "source": [
    "## I will now let it run for a few epochs, re-initialize the model and load the weights from the previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ad95f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.run(max_epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051407fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(lins, activation, optimizer, batch_size, init_lr, data, truth, Cinv, cost,\n",
    "             max_epochs = 20, saving = True, run_num = 4)\n",
    "model.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c024b9",
   "metadata": {},
   "source": [
    "## Other\n",
    "\n",
    "    -plot_ev: (int, default 3) how many epochs between plotting results\n",
    "    -train_fac: (float, default .8) fraction of the datset to be used for training\n",
    "    - some kwargs can be changed in both the model initialization and as a kwarg in model.run() (so you can change mid-run). The kwargs are 'lr', 'lr_decay', 'lr_min', 'max_epochs', 'saving', 'plot_ev', and 'batch_size'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13ffd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
